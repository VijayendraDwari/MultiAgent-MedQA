{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fHNDf1ymzhz"
      },
      "outputs": [],
      "source": [
        "# Install required libraries\n",
        "!pip install unsloth datasets langchain sentence-transformers gradio chromadb langchain-community TTS -q\n",
        "\n",
        "# Import necessary libraries\n",
        "import torch\n",
        "import gradio as gr\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastLanguageModel, FastVisionModel\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.llms.base import LLM\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from typing import List, Any, Optional\n",
        "from pydantic import PrivateAttr\n",
        "\n",
        "# Mount Google Drive for the models\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Paths to models\n",
        "phi4_model_path = \"/content/drive/My Drive/MedQA-Phi4_LoRA_Model/lora_model\"\n",
        "xray_model_path = \"/content/drive/My Drive/Llama3.2-Vision-radiology\"\n",
        "\n",
        "# Load Phi4 Model (Agent 1)\n",
        "phi4_model, phi4_tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=phi4_model_path,\n",
        "    max_seq_length=2048,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "FastLanguageModel.for_inference(phi4_model)\n",
        "\n",
        "from langchain.llms.base import LLM\n",
        "from pydantic import PrivateAttr\n",
        "\n",
        "class LangChainPhi4LLM(LLM):\n",
        "    \"\"\"\n",
        "    Custom wrapper to make the Phi4 model compatible with LangChain.\n",
        "    \"\"\"\n",
        "    _model: Any = PrivateAttr()\n",
        "    _tokenizer: Any = PrivateAttr()\n",
        "    max_new_tokens: int\n",
        "    device: str\n",
        "\n",
        "    def __init__(self, model, tokenizer, max_new_tokens=256, device=\"cuda\"):\n",
        "        super().__init__(max_new_tokens=max_new_tokens, device=device)\n",
        "        self._model = model\n",
        "        self._tokenizer = tokenizer\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom_phi4_llm\"\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:\n",
        "        \"\"\"\n",
        "        Generates a response for the given prompt using the Phi4 model.\n",
        "        \"\"\"\n",
        "        inputs = self._tokenizer([prompt], return_tensors=\"pt\", truncation=True, padding=True, max_length=2048).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self._model.generate(**inputs, max_new_tokens=self.max_new_tokens, use_cache=True)\n",
        "        return self._tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "wrapped_phi4_llm = LangChainPhi4LLM(model=phi4_model, tokenizer=phi4_tokenizer)\n",
        "\n",
        "# Load Vision Model (Agent 2)\n",
        "xray_model, xray_tokenizer = FastVisionModel.from_pretrained(\n",
        "    xray_model_path,\n",
        "    load_in_4bit=True,\n",
        "    use_gradient_checkpointing=\"unsloth\"\n",
        ")\n",
        "FastVisionModel.for_inference(xray_model)\n",
        "\n",
        "# Load PubMedQA dataset and create vector store\n",
        "dataset = load_dataset(\"bigbio/pubmed_qa\", name=\"pubmed_qa_labeled_fold0_source\", split=\"train\", trust_remote_code=True)\n",
        "docs = [{\"id\": f\"pubmed_qa_train_{i}\", \"text\": ex[\"LONG_ANSWER\"] or \"\"} for i, ex in enumerate(dataset)]\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "chunks = [{\"id\": f\"{d['id']}_chunk_{idx}\", \"text\": chunk} for d in docs for idx, chunk in enumerate(text_splitter.split_text(d[\"text\"]))]\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "texts = [x[\"text\"] for x in chunks]\n",
        "metadatas = [{\"source\": x[\"id\"]} for x in chunks]\n",
        "db = Chroma.from_texts(texts=texts, embedding=embeddings, metadatas=metadatas, persist_directory=\"chroma_store\")\n",
        "\n",
        "# Define Prompt and Retrieval Chain for Phi4 (Agent 1)\n",
        "prompt_template = \"\"\"You are a medical QA system.\n",
        "Use the following context to answer the question concisely without repeating the context or any lines in the final answer:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt = PromptTemplate(template=prompt_template, input_variables=[\"context\", \"question\"])\n",
        "retriever = db.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
        "\n",
        "# Initialize RetrievalQA with the wrapped Phi4 model\n",
        "phi4_rag = RetrievalQA.from_chain_type(\n",
        "    llm=wrapped_phi4_llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    chain_type_kwargs={\"prompt\": prompt}\n",
        ")\n",
        "\n",
        "# Define Vision Agent (Agent 2)\n",
        "def infer_xray(image):\n",
        "    instruction = \"You are an expert radiographer. Describe accurately and with details what you see in this image.\"\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "            {\"type\": \"image\", \"image\": image},\n",
        "            {\"type\": \"text\", \"text\": instruction}\n",
        "        ]}\n",
        "    ]\n",
        "    input_text = xray_tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n",
        "    inputs = xray_tokenizer(\n",
        "        image,\n",
        "        input_text,\n",
        "        add_special_tokens=False,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        output = xray_model.generate(\n",
        "            **inputs, max_new_tokens=256, use_cache=True, eos_token_id=xray_tokenizer.eos_token_id\n",
        "        )\n",
        "    # Extract and clean the relevant text\n",
        "    xray_text = xray_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return xray_text.strip()\n",
        "\n",
        "# Define Multi-Agent Logic\n",
        "def multi_agent_system(input_type, query=None, image=None):\n",
        "    if input_type == \"Chat\" and query:\n",
        "        # Agent 1 handles text-based queries\n",
        "        response = phi4_rag.invoke({\"query\": query})\n",
        "        raw_answer = response[\"result\"]\n",
        "        final_answer = raw_answer.split(\"Answer:\")[-1].strip() if \"Answer:\" in raw_answer else raw_answer.strip()\n",
        "        return f\"Chat Answer:\\n{final_answer}\"\n",
        "    elif input_type == \"X-ray\" and image is not None:\n",
        "        # Agent 2 processes the image and passes output to Agent 1\n",
        "        xray_text = infer_xray(image)\n",
        "        response = phi4_rag.invoke({\"query\": xray_text})\n",
        "        raw_answer = response[\"result\"]\n",
        "        final_answer = raw_answer.split(\"Answer:\")[-1].strip() if \"Answer:\" in raw_answer else raw_answer.strip()\n",
        "        return f\"X-ray Analysis:\\n{xray_text}\\n\\nGenerated Answer:\\n{final_answer}\"\n",
        "    else:\n",
        "        # Handle invalid inputs\n",
        "        return \"Please provide a valid query or image based on the selected input type.\"\n",
        "\n",
        "# Gradio Interface\n",
        "interface = gr.Interface(\n",
        "    fn=multi_agent_system,\n",
        "    inputs=[\n",
        "        gr.Radio([\"Chat\", \"X-ray\"], label=\"Select Input Type\"),\n",
        "        gr.Textbox(label=\"Enter your query (for Chat)\", placeholder=\"Type your query here...\"),\n",
        "        gr.Image(label=\"Upload X-ray Image (for X-ray)\", type=\"numpy\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Response\"),\n",
        "    title=\"Multi-Agent Medical System\",\n",
        "    description=\"A system where one agent handles text-based medical QA and another processes X-ray images to generate insights.\"\n",
        ")\n",
        "\n",
        "# Launch the app\n",
        "interface.launch(share=True)\n"
      ]
    }
  ]
}